{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install opencv-python\n",
    "# !pip install dlib\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the face detector from dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the facial landmarks predictor from dlib\n",
    "predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')\n",
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract facial features for a single image\n",
    "def extract_features(image_path):\n",
    "    # <class 'numpy.ndarray'>\n",
    "    if isinstance(image_path, np.ndarray):\n",
    "        image = image_path\n",
    "    else:\n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path)\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Detect faces in the grayscale image\n",
    "    faces = detector(gray)\n",
    "    # Initialize an array to store the facial features\n",
    "    features = []\n",
    "    # Iterate over detected faces\n",
    "    for face in faces:\n",
    "        # Get the facial landmarks\n",
    "        landmarks = predictor(gray, face)\n",
    "        # Initialize an array to store the landmarks\n",
    "        landmarks_array = np.zeros((68, 2), dtype=np.float32)\n",
    "        # Iterate over the facial landmarks\n",
    "        for n in range(68):\n",
    "            x = landmarks.part(n).x\n",
    "            y = landmarks.part(n).y\n",
    "            # Store the landmark coordinates in the array\n",
    "            landmarks_array[n] = (x, y)\n",
    "        # Append the landmarks array to the features array\n",
    "        features.append(landmarks_array)\n",
    "    # Return the extracted features\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing training images\n",
    "train_dir = './dataset'\n",
    "\n",
    "# List to store the extracted features and corresponding labels\n",
    "X = []\n",
    "y = []\n",
    "images_with_face = []\n",
    "\n",
    "# Iterate over the training images\n",
    "for person_name in os.listdir(train_dir):\n",
    "    person_dir = os.path.join(train_dir, person_name)\n",
    "    if os.path.isdir(person_dir):\n",
    "        # Iterate over the images of the current person\n",
    "        for image_name in os.listdir(person_dir):\n",
    "            image_path = os.path.join(person_dir, image_name)\n",
    "            # Extract features from the image\n",
    "            features = extract_features(image_path)\n",
    "            if len(features):\n",
    "                # Add the features to the list\n",
    "                X.extend(features)\n",
    "                # Add the label to the list\n",
    "                y.extend([person_name] * len(features))\n",
    "                # if there is a face detected in the image, print the image path\n",
    "                images_with_face.append(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first person in the dataset, plot all the images with faces and their corresponding facial landmarks in a grid\n",
    "person_name = os.listdir(train_dir)[0]\n",
    "person_dir = os.path.join(train_dir, person_name)\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "# Number of columns and rows based on number of images\n",
    "columns = 6\n",
    "rows = 10\n",
    "\n",
    "# Limit the number of images to be plotted based on the grid size\n",
    "num_images = min(len(images_with_face), columns * rows)\n",
    "\n",
    "for i in range(1, num_images + 1):\n",
    "    image_path = images_with_face[i-1]  # Adjust the index to start from 0\n",
    "    image = mpimg.imread(image_path)\n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(img_gray)\n",
    "    for face in faces:\n",
    "        landmarks = predictor(img_gray, face)\n",
    "        for n in range(68):\n",
    "            Cx = landmarks.part(n).x\n",
    "            Cy = landmarks.part(n).y\n",
    "            cv2.circle(image, (Cx, Cy), 10, (0, 255, 255), -1)\n",
    "\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a chart to show the number of images with faces\n",
    "# create a figure with 2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# plot the number of images with faces\n",
    "ax1.bar(np.unique(y), [y.count(i) for i in np.unique(y)])\n",
    "ax1.set_title('Number of images with faces')\n",
    "ax1.set_xlabel('Person')\n",
    "ax1.set_ylabel('Number of images')\n",
    "# plot a random image with a face\n",
    "ax2.imshow(mpimg.imread(images_with_face[np.random.randint(len(images_with_face))]))\n",
    "ax2.set_title('Random image with a face')\n",
    "ax2.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the feature list to a NumPy array\n",
    "X = np.array(X)\n",
    "# Flatten the 2D array of features to 1D\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "# Convert the label list to a NumPy array\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize an SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_face_picture(image_path):\n",
    "    # Extract features from the test image\n",
    "    image_features = extract_features(image_path)\n",
    "    # Flatten the 2D array of features to 1D\n",
    "    image_features = np.array(image_features).reshape(1, -1)\n",
    "    predicted_label = None\n",
    "    if image_features.shape[1]:\n",
    "        # Extract the label of the test image\n",
    "        predicted_label = svm.predict(image_features)[0]\n",
    "    # Return the name of the predicted label\n",
    "    return predicted_label\n",
    "\n",
    "def identify_face_video(video_path):\n",
    "    # Open the video file for reading\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    # List to store the identified people\n",
    "    identified_people = []\n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frame_count = 0\n",
    "    # Read the video frame by frame\n",
    "    for _ in tqdm(range(total_frames)):\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % 10 == 0:\n",
    "            # Extract features from the current frame\n",
    "            frame_features = extract_features(frame)\n",
    "            # If any faces are detected in the frame\n",
    "            if len(frame_features) > 0:\n",
    "                # Flatten the 2D array of features to 1D\n",
    "                frame_features = np.array(frame_features).reshape(len(frame_features), -1)\n",
    "                # Predict the labels for the frame features\n",
    "                predicted_labels = svm.predict(frame_features)\n",
    "                # Add the predicted labels to the identified people list\n",
    "                identified_people.extend(predicted_labels)\n",
    "        frame_count += 1\n",
    "    \n",
    "    # Release the video capture object\n",
    "    video.release()\n",
    "\n",
    "    # unique identified people\n",
    "    unique_identified_people = set(identified_people)\n",
    "    # count of identified people\n",
    "    data = [(i, identified_people.count(i)) for i in unique_identified_people]\n",
    "    if not len(data):\n",
    "        return 'unknown'\n",
    "    # get highest count and the person\n",
    "    max_count = max(data, key=lambda x: x[1])\n",
    "    return max_count[0]  # person label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORY: ./input/ ['Gallery', 'GBWhatsApp_Images', 'GBWhatsApp_Video']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORY: ./input/Gallery ['owner']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORY: ./input/Gallery\\owner ['Haseeb', 'Muhammad Mustafa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORY: ./input/Gallery\\owner\\Haseeb []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORY: ./input/Gallery\\owner\\Muhammad Mustafa []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [00:01<00:00, 250.69it/s]\n",
      "100%|██████████| 1382/1382 [00:05<00:00, 273.49it/s]\n",
      "100%|██████████| 348/348 [00:01<00:00, 236.57it/s]\n",
      "100%|██████████| 630/630 [00:02<00:00, 238.19it/s]\n",
      "100%|██████████| 966/966 [00:03<00:00, 279.76it/s]\n",
      "100%|██████████| 66/66 [00:00<00:00, 297.30it/s]\n",
      "100%|██████████| 637/637 [00:02<00:00, 288.91it/s]\n",
      "100%|██████████| 201/201 [00:00<00:00, 300.45it/s]\n",
      "100%|██████████| 266/266 [00:00<00:00, 293.27it/s]\n",
      "100%|██████████| 807/807 [00:02<00:00, 307.87it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 305.87it/s]\n",
      "100%|██████████| 286/286 [00:00<00:00, 305.22it/s]\n",
      "100%|██████████| 25/25 [00:26<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORY: ./input/GBWhatsApp_Images ['Private', 'Sent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6705/6705 [22:14<00:00,  5.02it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORY: ./input/GBWhatsApp_Images\\Private []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORY: ./input/GBWhatsApp_Images\\Sent []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 561/3425 [01:50<27:36,  1.73it/s]"
     ]
    }
   ],
   "source": [
    "test_dir = './input/'\n",
    "output_dir = './output'\n",
    "images_dir = os.path.join(output_dir, 'images')\n",
    "videos_dir = os.path.join(output_dir, 'videos')\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "# Create the images directory if it doesn't exist\n",
    "if not os.path.exists(images_dir):\n",
    "    os.mkdir(images_dir)\n",
    "\n",
    "# Create the videos directory if it doesn't exist\n",
    "if not os.path.exists(videos_dir):\n",
    "    os.mkdir(videos_dir)\n",
    "\n",
    "# Iterate over the files in the test directory and its subdirectories\n",
    "for root, dirs, files in os.walk(test_dir):\n",
    "    print(\"DIRECTORY:\", root, dirs)\n",
    "    for file_name in tqdm(files):\n",
    "        try:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            # Check if the file is an image\n",
    "            if file_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                # Process the image\n",
    "                predicted_label = identify_face_picture(file_path)\n",
    "                if predicted_label:\n",
    "                    label_images_dir = os.path.join(images_dir, predicted_label)\n",
    "                    if not os.path.exists(label_images_dir):\n",
    "                        os.mkdir(label_images_dir)\n",
    "                    os.rename(file_path, os.path.join(label_images_dir, file_name))\n",
    "                else:\n",
    "                    unknown_images_dir = os.path.join(images_dir, 'unknown')\n",
    "                    if not os.path.exists(unknown_images_dir):\n",
    "                        os.mkdir(unknown_images_dir)\n",
    "                    os.rename(file_path, os.path.join(unknown_images_dir, file_name))\n",
    "\n",
    "            # Check if the file is a video\n",
    "            elif file_name.endswith(('.mp4')):\n",
    "                # Process the video frames\n",
    "                predicted_label = identify_face_video(file_path)\n",
    "                if predicted_label:\n",
    "                    label_videos_dir = os.path.join(videos_dir, predicted_label)\n",
    "                    if not os.path.exists(label_videos_dir):\n",
    "                        os.mkdir(label_videos_dir)\n",
    "                    os.rename(file_path, os.path.join(label_videos_dir, file_name))\n",
    "                else:\n",
    "                    unknown_videos_dir = os.path.join(videos_dir, 'unknown')\n",
    "                    if not os.path.exists(unknown_videos_dir):\n",
    "                        os.mkdir(unknown_videos_dir)\n",
    "                    os.rename(file_path, os.path.join(unknown_videos_dir, file_name))\n",
    "        except Exception as e:\n",
    "            # print(\"error: file_path\", file_path, e)\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
